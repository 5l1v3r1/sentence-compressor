{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project on natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import compression_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = compression_data.load_compression_data(\"compression-data.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that convert array of tokkens to string, either compressed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Luis Suarez was spotted in London'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prase_array_to_string(prase, compressed = False):\n",
    "    return ' '.join([t.form for t in filter(lambda t: t.label == 0 if compressed else True, prase)])\n",
    "prase_array_to_string(data[2], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the word2vec embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make embeding part of every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "unk = np.zeros(300)\n",
    "TokenWithEmb = namedtuple(\"TokenWithEmb\", \"form tag stem label emb\")\n",
    "data_with_emb = []\n",
    "for phrase in data:\n",
    "    new_phrase = []\n",
    "    for word in phrase:\n",
    "        new_phrase.append(TokenWithEmb(word.form, word.tag, word.stem, word.label, model[word.form]\n",
    "                                  if word.form in model else unk))\n",
    "    data_with_emb.append(new_phrase)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Split the dataset into train, test and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_with_dev, test = train_test_split(data_with_emb, test_size = 2000, random_state = 0)\n",
    "train, dev = train_test_split(train_with_dev, test_size = 2000, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000, 6000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test), len(dev), len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to iterate over phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function return one phrase at each yield, first field is matrix of embedings, second is vector of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate (dataset, shuffle = True):\n",
    "    if shuffle:\n",
    "        for i in np.random.permutation(len(dataset)):\n",
    "            yield np.vstack(map(lambda token: token.emb, dataset[i])), np.array(map(lambda token: token.label, dataset[i]))\n",
    "    else:\n",
    "        for i in range(len(dataset)):\n",
    "            yield np.vstack(map(lambda token: token.emb, dataset[i])), np.array(map(lambda token: token.label, dataset[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network consist of 2 Recurent GRU layers, each one has forward and backward component \n",
    "(the one traverce phrase from the begining and the other from the end). Between these 2 dropout layer.\n",
    "After recurent layer there are dence layer with softmax nonlinearity. Othere layers are just reshapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.matrix('input','float32')\n",
    "Y = T.ivector('target')\n",
    "embeding_size = 300\n",
    "rnn_size = 100\n",
    "hid_size = 200\n",
    "def network_architecture():\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, embeding_size), input_var = X)\n",
    "    \n",
    "    l_sin = lasagne.layers.ReshapeLayer(l_in, shape = (1, -1, embeding_size))\n",
    "    \n",
    "    l_rnn_forward = lasagne.layers.GRULayer(l_sin, grad_clipping = 100, num_units = rnn_size)\n",
    "    l_rnn_backward = lasagne.layers.GRULayer(l_sin, grad_clipping = 100, num_units = rnn_size, backwards=True)\n",
    "    l_rnn = lasagne.layers.ConcatLayer([l_rnn_forward, l_rnn_backward], axis = 2)\n",
    "    \n",
    "    l_dropout = lasagne.layers.DropoutLayer(l_rnn, p = 0.25)\n",
    "    \n",
    "    l_rnn_forward = lasagne.layers.GRULayer(l_dropout, grad_clipping = 100, num_units = rnn_size)\n",
    "    l_rnn_backward = lasagne.layers.GRULayer(l_dropout, grad_clipping = 100, num_units = rnn_size, backwards=True)\n",
    "    l_rnn = lasagne.layers.ConcatLayer([l_rnn_forward, l_rnn_backward], axis = 2)\n",
    "     \n",
    "    l_shp = lasagne.layers.ReshapeLayer(l_rnn, shape = (-1, 2 * rnn_size))    \n",
    "    \n",
    "    l_hid = lasagne.layers.DenseLayer(l_shp, num_units = hid_size)\n",
    "    \n",
    "    l_dropout = lasagne.layers.DropoutLayer(l_hid, p=0.25)\n",
    "\n",
    "    l_den = lasagne.layers.DenseLayer(l_dropout, num_units = 2, nonlinearity=lasagne.nonlinearities.identity)\n",
    "    l_sden = lasagne.layers.ReshapeLayer(l_den, shape = (-1, 2))\n",
    "    l_out = lasagne.layers.NonlinearityLayer(l_sden, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "network = network_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function for training and testing the network.  Network will be thrained using adam method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_network(l_out, learnig_rate):\n",
    "    weights = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "    network_output_det = lasagne.layers.get_output(l_out, deterministic = True)\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(network_output, Y).mean()\n",
    "    loss_det = lasagne.objectives.categorical_crossentropy(network_output_det, Y).mean()\n",
    "\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate = learnig_rate)\n",
    "\n",
    "\n",
    "    #training functio\n",
    "    train = theano.function([X, Y], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #loss function\n",
    "    compute_cost = theano.function([X, Y], loss_det, allow_input_downcast=True)\n",
    "\n",
    "    #prediction function\n",
    "    probs = theano.function([X], network_output_det,allow_input_downcast=True)\n",
    "    \n",
    "    return train, compute_cost, probs\n",
    "\n",
    "train_func, compute_cost_func, probs_func = compile_network(network, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cycle. Cycle consist of n_epoch. In each epoch network trained on every phrase in train set and score computed on dev set. The network with highest dev score will be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch 0: train loss - 0.500007, dev loss - 0.458575\n",
      "Epoch 1: train loss - 0.421191, dev loss - 0.432249\n",
      "Epoch 2: train loss - 0.370535, dev loss - 0.435129\n",
      "Epoch 3: train loss - 0.319932, dev loss - 0.438525\n",
      "Epoch 4: train loss - 0.268731, dev loss - 0.462942\n",
      "Epoch 5: train loss - 0.232072, dev loss - 0.528093\n"
     ]
    }
   ],
   "source": [
    "def trainig_cycle(train_func, compute_cost_func, net, n_epochs):\n",
    "    print(\"Training ...\")\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    best_score = 10000000\n",
    "    best_weight = None\n",
    "    train_epoch_loss = []\n",
    "    dev_epoch_loss = []\n",
    "    for epoch in range(n_epochs):    \n",
    "        train_losses = []\n",
    "        dev_losses = []\n",
    "\n",
    "        for X, Y in iterate(train):\n",
    "            tr_loss = train_func(X, Y)\n",
    "            train_losses.append(tr_loss)\n",
    "\n",
    "        for X, Y in iterate(dev):        \n",
    "            dev_loss = compute_cost_func(X, Y)\n",
    "            dev_losses.append(dev_loss)\n",
    "        \n",
    "        if dev_loss < best_score:\n",
    "            best_weight = lasagne.layers.get_all_param_values(net)\n",
    "            best_score = dev_loss\n",
    "\n",
    "        print \"Epoch %d: train loss - %f, dev loss - %f\" % (epoch, np.mean(train_losses), np.mean(dev_losses))\n",
    "        train_epoch_loss.append(np.mean(train_losses))\n",
    "        dev_epoch_loss.append(np.mean(dev_losses))\n",
    "    return train_epoch_loss, dev_epoch_loss, best_weight\n",
    "\n",
    "train_epoch_loss, dev_epoch_loss, best_weight = trainig_cycle(train_func, compute_cost_func, network, 6)\n",
    "lasagne.layers.set_all_param_values(network, best_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy and auc in every phrase and report the avearag. Also compute the number of phrases that match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avearage auc 0.874680791633, avearage accuracy 0.803137810774\n",
      "Number of phrases that match 283/2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "aucs = []\n",
    "accuracys = []\n",
    "count_total_match = 0\n",
    "\n",
    "for X, Y in iterate(test, False):\n",
    "    probs = probs_func(X)\n",
    "    predictions = (probs[:,1] > 0.5).astype(int)\n",
    "    aucs.append(roc_auc_score(Y, probs[:, 1]))\n",
    "    accuracys.append(accuracy_score(Y, predictions))\n",
    "    count_total_match += np.array_equal(Y, predictions)\n",
    "\n",
    "print \"Avearage auc %s, avearage accuracy %s\" % (np.mean(aucs), np.mean(accuracys))\n",
    "print \"Number of phrases that match %s/%s\" % (count_total_match, len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the phrase along with it`s compression and predicted compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: 'New York Aug 28 Actor Alec Baldwin reportedly attacked a photographer who was following him and his wife Hilaria Thomas here'\n",
      "True compression: 'Alec Baldwin attacked a photographer'\n",
      "Compression predicted: 'Alec Baldwin reportedly attacked a photographer'\n"
     ]
    }
   ],
   "source": [
    "from itertools import compress\n",
    "def test_phrase(number):\n",
    "    for X, Y in iterate(test[number:(number+1)], False):\n",
    "        probs = probs_func(X)\n",
    "        predictions = (probs[:,1] > 0.5).astype(int)\n",
    "        phrase = prase_array_to_string(test[number])\n",
    "        phrase_compressed = prase_array_to_string(test[number], True)\n",
    "        phrase_compression_prediction = ' '.join(map(lambda x: x.form, compress(test[number], 1 - predictions)))\n",
    "        print \"Phrase: '%s'\" % phrase\n",
    "        print \"True compression: '%s'\" % phrase_compressed\n",
    "        print \"Compression predicted: '%s'\" % phrase_compression_prediction\n",
    "\n",
    "test_phrase(19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
