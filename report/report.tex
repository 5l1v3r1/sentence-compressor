%\documentclass[a4paper,12pt,oneside,draft]{article}
\documentclass[a4paper,12pt,oneside]{article}

% In the original writelatex tamplate
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

% By LoCigno
\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{csvsimple}
\usepackage{color}
\usepackage{url}
\usepackage{hyperref} 
\usepackage{cleveref}

% By Davide
\usepackage{comment}
\usepackage{booktabs}
\usepackage{color}

%Variables macros
\newcommand{\DefineVar}[2]{%
  \expandafter\newcommand\csname var-#1\endcsname{#2}%
} 
\newcommand{\var}[1]{\csname var-#1\endcsname}

\usepackage{courier}
\newcommand{\mono}[1]{\texttt{#1}}
%\newcommand{\mono}[1]{\texttt{\textbf{#1}}}

\title{Sentence compressor}

\author{Aliaksandr Siarohin}

\date{\today}

\begin{document}
%\maketitle
\makeatletter  % populates \@title, \@author, \@date
\begin{titlepage}
      \centering
      ~~~~~~~~~~~~~\\[-30mm]
      \includegraphics[keepaspectratio=true, width=7cm]{bg_eng_1r.jpg} \\[10mm]

     {
     \large \bfseries Master Degree in Computer Science\\[3mm] 
     Natural Language Processing\\[3mm]
     AA 2015-2016
     }\\[10mm]

     %--------------------------------
     % Set the title, author, and date
     % 

     \vspace{0.5cm}
     {
     \Large \bfseries \textcolor{blue}{\@title} \par
     }
     \vspace{0.5cm}
%      {
%      \large {Group N. 1} \par
%      }
     \vspace{0.2cm}

     {\large {\@author}}
     \\ \vspace{.2cm}
     \@date

     \vspace{0.6cm}

    %-----------------------------------

\begin{abstract}

\textit{
  This is a description of project on natural language processing.
}


\end{abstract}

\end{titlepage}



\section {Introduction to the designed system}
In this project, I implement a neural network solution to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions. I use architecture similar to one proposed in \cite{lstm}. My network is multiple LSTM layers stacked together. This network takes words embeddings from word2vec as an input and produces the probability of token deletion decision.

\section {The analytic model}

\subsection{Pre-processing}
The data were initially in json format. To parse json I use script provided by teaching assistant. After parsing the data become the list of lists with tokens. Each token is tuple with 4 field (word, tag, stem, token deletion decision). I populate this tuple with an additional field, word embedding. This embedding comes from google-news word2vec \url{https://code.google.com/archive/p/word2vec/}.

\subsection{Formal description of neural network architecture}
I use LSTM network for this problem. The input to the network is word embedding and one hot encoding of word tag. Then there is 2 bidirectional LSTM layers separated by dropout layer. Then there is a dense layer with a dropout. And finally dense layer with softmax non-linearity. The network structure is in \cref{fig:network}.

\begin{figure}[t]%
	\centering
	\includegraphics[width=230pt]{compression_network}
	\caption{The network architecture}%
	\label{fig:network}%
\end{figure}


\section{Software description}
To implement this project I use python library's theano+lasagne. This framework can build a graph of execution, and then evaluate it on gpu.

\subsection{Used Functions}
I use different network layers from lasagne library (LSTMLayer, DropoutLayer, DenseLayer ...). Also, I use adam function from lasagne library for fitting the network.

\section{Experiment Description}

\subsection{Data}
Data consist of 10000 sentences. For each word in sentence there is stem form of this word and tag of this word. Each word has associated token deletion decisions (0 or 1). For the experiment I split the data in 3 sets: train, dev and test. The test set consists of 1000 sentences from the beginning, train and dev set is a random split of the rest 9000 sentences, 1000 for dev and 8000 for train.


\subsection{Training process}
I train the network using adam method. The batch in my case is 1 sentence. The sentences are feed to network in random order. The training process takes 4 epochs. After each 1000 sentences, I compute score for dev set and save network parameters. The network with the best score on validation set will be resulting.

\subsection{Network parameters}
The size of word embedding is 300 (Because in google-news dataset it is 300). The number of units in LSTM layer is 200, as well as in tag embedding. The size of last dense layer is also 200. The last layer gives us probability, so to get 0, 1 predictions we need some threshold. In my case this threshold maximize the f1 score on dev set, and it is equal to 0.505582.

\subsection{Source code}
Sorce code is single ipython notebook. I upload it on github. You can view it using the following link: \url{https://github.com/AliaksandrSiarohin/sentence-compressor}.

\section{Result presentation}
The result score on the test set:

\begin{itemize}
  \item Per token accuracy: 0.827522
  \item Per token f1 score: 0.768665
  \item Fraction of right compression: 0.163000
\end{itemize}

The learning curves is in \cref{fig:lcur}. The Precision/Recall curve is in \cref{fig:pr}.

\begin{figure}[t]%
	\centering
	\includegraphics[width=\columnwidth]{learning_curves}
	\caption{The learning curves on train and dev sets}%
	\label{fig:lcur}%
\end{figure}


\begin{figure}[t]%
	\centering
	\includegraphics[width=\columnwidth]{precision_recal_curve}
	\caption{The Precision/Recall curve}%
	\label{fig:pr}%
\end{figure}

The table with compression examples is in \cref{table:gc} for good compression, and \cref{table:b} for bad compression.



\section {Comparison with other solutions}
This problem has already been addressed in \cite{lstm} and \cite{gaze_lstm}.

The first one use approche similar to my own, but with 3 LSTM layers and additional features such as:
\begin{itemize}
\item The embedding vector for the parent word
\item The label predicted for the last word
\item A bit indicating whether the parent word has already been seen and kept in the compression
\item A bit indicating whether the parent word has already  been  seen  but  discarded
\item A  bit  indicating  whether  the  parent  word comes later in the input
\end{itemize}
In this paper, they also use a larger corpus for training about 2 million sentences. Their f1 score 0.81.

The second one also using 3 LSTM layers and same amount of data as my model, but they take eye-tracking recording as an additional feature. Their f1 score 0.8097.


\section {Conclusion}
In this project, I implement a neural network solution to deletion-based sentence compression. Resulting network shows good results on phrases with one subject and predicate. But not very good at phrases without subject, as well as complex phrases with 2 or more subjects.


\bibliographystyle{IEEEtran}
\bibliography{references}
\newpage 

\begin{table}
\centering
\caption{Good compression}
\label{table:gc}
\csvreader[tabular=p{150pt}p{100pt}p{100pt},
    table head=\toprule Starting sentence & Reference  &  Predicted \\\midrule,
    table foot=\bottomrule,
    filter={\value{csvrow}<6}
]%
{good_compressions.csv}{Phrase=\Phrase,True=\True, Predicted=\Predicted}%
{\Phrase & \True & \Predicted \\\midrule}%
\end{table}


\begin{table}
\centering
\caption{Bad compression}
\label{table:b}
\csvreader[tabular=p{150pt}p{100pt}p{100pt},
    table head=\toprule Starting sentence & Reference  &  Predicted \\\midrule,
    table foot=\bottomrule,
    filter={\value{csvrow}<6}
]%
{bad_compressions.csv}{Phrase=\Phrase,True=\True, Predicted=\Predicted}%
{\Phrase & \True & \Predicted \\\midrule}%
\end{table}

\end{document}