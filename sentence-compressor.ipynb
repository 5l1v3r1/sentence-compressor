{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project on natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import compression_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = compression_data.load_compression_data(\"compression-data.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that convert array of tokkens to string, either compressed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Luis Suarez was spotted in London'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prase_array_to_string(prase, compressed = False):\n",
    "    return ' '.join([t.form for t in filter(lambda t: t.label == 0 if compressed else True, prase)])\n",
    "prase_array_to_string(data[2], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the word2vec embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make embeding part of every token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "unk = np.zeros(300)\n",
    "TokenWithEmb = namedtuple(\"TokenWithEmb\", \"form tag stem label emb\")\n",
    "data_with_emb = []\n",
    "for phrase in data:\n",
    "    new_phrase = []\n",
    "    for word in phrase:\n",
    "        new_phrase.append(TokenWithEmb(word.form, word.tag, word.stem, word.label, model[word.form]\n",
    "                                  if word.form in model else unk))\n",
    "    data_with_emb.append(new_phrase)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign number to every tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tags = [token.tag for phrase in data_with_emb for token in phrase]\n",
    "tags_dict = {tag:i for i, tag in enumerate(set(all_tags))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Split the dataset into train, test and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_with_dev, test = train_test_split(data_with_emb, test_size = 500, random_state = 0)\n",
    "train, dev = train_test_split(train_with_dev, test_size = 1000, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000, 8500)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test), len(dev), len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to iterate over phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function return one phrase at each yield, first field is matrix of embedings, second vector of tags encodings, third is vector of answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate (dataset, shuffle = True):\n",
    "    perm = np.random.permutation(len(dataset)) if shuffle else range(len(dataset))\n",
    "    for i in perm:\n",
    "        yield (np.vstack(map(lambda token: token.emb, dataset[i])),\n",
    "               np.array(map(lambda token: tags_dict[token.tag], dataset[i])),\n",
    "               np.array(map(lambda token: token.label, dataset[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_emb = T.matrix('input','float32')\n",
    "X_tag = T.ivector('tag')\n",
    "Y = T.ivector('target')\n",
    "embeding_size = 300\n",
    "tag_emb_size = 200\n",
    "rnn_size = 500\n",
    "hid_size = 500\n",
    "def network_architecture():\n",
    "    #Input layer for word2vec embeding\n",
    "    l_emb_in = lasagne.layers.InputLayer(shape=(None, embeding_size), input_var = X_emb)\n",
    "    \n",
    "    #Input layer for tag id\n",
    "    l_tag_in = lasagne.layers.InputLayer(shape=(None, ),input_var = X_tag)\n",
    "    \n",
    "    #Embeding layer for tags\n",
    "    l_emb = lasagne.layers.EmbeddingLayer(l_tag_in, input_size=len(tags_dict), output_size = tag_emb_size)\n",
    "    \n",
    "    #Concat word2vec embeding and tags embeding\n",
    "    l_concat = lasagne.layers.ConcatLayer([l_emb_in, l_emb], axis = 1)\n",
    "    \n",
    "    #Reshape previous layer to fit RNN layer format \n",
    "    l_reshape = lasagne.layers.ReshapeLayer(l_concat, shape = (1, -1, embeding_size + tag_emb_size))\n",
    "    \n",
    "    #First recurent forward and backward layer and concatenation of them\n",
    "    l_rnn_forward = lasagne.layers.LSTMLayer(l_reshape, grad_clipping = 100, num_units = rnn_size)\n",
    "    l_rnn_backward = lasagne.layers.LSTMLayer(l_reshape, grad_clipping = 100, num_units = rnn_size, backwards=True)\n",
    "    l_rnn = lasagne.layers.ConcatLayer([l_rnn_forward, l_rnn_backward], axis = 2)\n",
    "    \n",
    "    \n",
    "    l_dropout = lasagne.layers.DropoutLayer(l_rnn, p=0.2)\n",
    "    \n",
    "    #Second recurent layer\n",
    "    l_rnn_forward = lasagne.layers.LSTMLayer(l_dropout, grad_clipping = 100, num_units = rnn_size)\n",
    "    l_rnn_backward = lasagne.layers.LSTMLayer(l_dropout, grad_clipping = 100, num_units = rnn_size, backwards=True)\n",
    "    l_rnn = lasagne.layers.ConcatLayer([l_rnn_forward, l_rnn_backward], axis = 2)\n",
    "    \n",
    "    #Reshape to fit dence layer format\n",
    "    l_shp = lasagne.layers.ReshapeLayer(l_rnn, shape = (-1, 2 * rnn_size))    \n",
    "    \n",
    "    #Dence layer with dropout\n",
    "    l_hid = lasagne.layers.DenseLayer(l_shp, num_units = hid_size)    \n",
    "    l_dropout = lasagne.layers.DropoutLayer(l_hid, p=0.1)\n",
    "    \n",
    "    #Dence layer with softmax nonlinerity, for final answer\n",
    "    l_den = lasagne.layers.DenseLayer(l_dropout, num_units = 2, nonlinearity=lasagne.nonlinearities.identity)\n",
    "    l_sden = lasagne.layers.ReshapeLayer(l_den, shape = (-1, 2))\n",
    "    l_out = lasagne.layers.NonlinearityLayer(l_sden, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "network = network_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function for training and testing the network.  Network will be thrained using adam method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_network(l_out, learnig_rate):\n",
    "    weights = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "    network_output_det = lasagne.layers.get_output(l_out, deterministic = True)\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(network_output, Y).mean()\n",
    "    loss_det = lasagne.objectives.categorical_crossentropy(network_output_det, Y).mean()\n",
    "\n",
    "    updates = lasagne.updates.adam(loss, weights, learning_rate = learnig_rate)\n",
    "\n",
    "\n",
    "    #training functio\n",
    "    train = theano.function([X_emb, X_tag, Y], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "    #loss function\n",
    "    compute_cost = theano.function([X_emb, X_tag, Y], loss_det, allow_input_downcast=True)\n",
    "\n",
    "    #prediction function\n",
    "    probs = theano.function([X_emb, X_tag], network_output_det, allow_input_downcast=True)\n",
    "    \n",
    "    return train, compute_cost, probs\n",
    "\n",
    "train_func, compute_cost_func, probs_func = compile_network(network, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training cycle. Cycle consist of n_epoch. In each epoch network trained on every phrase in train set and score computed on dev set. The network with highest dev score will be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch 0: train loss - 0.495334, dev loss - 0.435520\n",
      "Epoch 1: train loss - 0.394671, dev loss - 0.402078\n",
      "Epoch 2: train loss - 0.346510, dev loss - 0.409115\n",
      "Epoch 3: train loss - 0.288222, dev loss - 0.435973\n",
      "Epoch 4: train loss - 0.231198, dev loss - 0.467889\n",
      "Epoch 5: train loss - 0.202118, dev loss - 0.551481\n"
     ]
    }
   ],
   "source": [
    "def trainig_cycle(train_func, compute_cost_func, net, n_epochs):\n",
    "    print(\"Training ...\")\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    best_score = 10000000\n",
    "    best_weight = None\n",
    "    train_epoch_loss = []\n",
    "    dev_epoch_loss = []\n",
    "    for epoch in range(n_epochs):    \n",
    "        train_losses = []\n",
    "        dev_losses = []\n",
    "\n",
    "        for X_emb, X_tag, Y in iterate(train):\n",
    "            tr_loss = train_func(X_emb, X_tag,  Y)\n",
    "            train_losses.append(tr_loss)\n",
    "\n",
    "        for X_emb, X_tag, Y in iterate(dev):        \n",
    "            dev_loss = compute_cost_func(X_emb, X_tag,  Y)\n",
    "            dev_losses.append(dev_loss)\n",
    "        \n",
    "        if dev_loss < best_score:\n",
    "            best_weight = lasagne.layers.get_all_param_values(net)\n",
    "            best_score = dev_loss\n",
    "\n",
    "        print \"Epoch %d: train loss - %f, dev loss - %f\" % (epoch, np.mean(train_losses), np.mean(dev_losses))\n",
    "        train_epoch_loss.append(np.mean(train_losses))\n",
    "        dev_epoch_loss.append(np.mean(dev_losses))\n",
    "    return train_epoch_loss, dev_epoch_loss, best_weight\n",
    "\n",
    "train_epoch_loss, dev_epoch_loss, best_weight = trainig_cycle(train_func, compute_cost_func, network, 6)\n",
    "lasagne.layers.set_all_param_values(network, best_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy and auc in every phrase and report the avearag. Also compute the number of phrases that match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avearage auc 0.900689427125, avearage accuracy 0.834544672925\n",
      "Number of phrases that match 103/500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "aucs = []\n",
    "accuracys = []\n",
    "count_total_match = 0\n",
    "\n",
    "for X_emb, X_tag, Y in iterate(test, False):\n",
    "    probs = probs_func(X_emb, X_tag)\n",
    "    predictions = (probs[:,1] > 0.5).astype(int)\n",
    "    aucs.append(roc_auc_score(Y, probs[:, 1]))\n",
    "    accuracys.append(accuracy_score(Y, predictions))\n",
    "    count_total_match += np.array_equal(Y, predictions)\n",
    "\n",
    "print \"Avearage auc %s, avearage accuracy %s\" % (np.mean(aucs), np.mean(accuracys))\n",
    "print \"Number of phrases that match %s/%s\" % (count_total_match, len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the phrase along with it`s compression and predicted compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: 'Montreal-based international flight training specialist CAE Inc. said Wednesday it has won $ 140 million in new military contracts including options'\n",
      "True compression: 'CAE Inc. has won $ 140 million in military contracts'\n",
      "Compression predicted: 'CAE Inc. has won $ 140 million in military contracts'\n"
     ]
    }
   ],
   "source": [
    "from itertools import compress\n",
    "def test_phrase(number):\n",
    "    for X_emb, X_tag, Y in iterate(test[number:(number+1)], False):\n",
    "        probs = probs_func(X_emb, X_tag)\n",
    "        predictions = (probs[:,1] > 0.5).astype(int)\n",
    "        phrase = prase_array_to_string(test[number])\n",
    "        phrase_compressed = prase_array_to_string(test[number], True)\n",
    "        phrase_compression_prediction = ' '.join(map(lambda x: x.form, compress(test[number], 1 - predictions)))\n",
    "        print \"Phrase: '%s'\" % phrase\n",
    "        print \"True compression: '%s'\" % phrase_compressed\n",
    "        print \"Compression predicted: '%s'\" % phrase_compression_prediction\n",
    "\n",
    "test_phrase(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
